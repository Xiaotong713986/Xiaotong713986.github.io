<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/images/favicon.jpg">

    <title>Posts-FMC</title>
    
    <link rel="stylesheet" href="/css/style.css">
</head><body>
<header class="site-header">
    <div class="inner">
        <div class="site-title slide-horizontal-invert">
            
            <a href="/"> FMC-IML-Lab </a>
            
        </div>
        <input type="checkbox" id="check-menu" class="check-menu" hidden="">
        <label for="check-menu" class="nav-btn hide-desktop" aria-label="Open/close menu"><span></span></label>
        <nav class="site-nav hide-mobile slide-horizontal" role="navigation" aria-label="main navigation">
            <ul class="site-menu">
                
                
                
                
                <li >
                    <a href="/" >
                        <span>Home</span>
                    </a>
                </li>
                
                
                
                <li >
                    <a href="/group/" >
                        <span>Group</span>
                    </a>
                </li>
                
                
                
                <li >
                    <a href="/pub/" >
                        <span>Publications</span>
                    </a>
                </li>
                
            </ul>
        </nav>
    </div>
</header>



<div class="card" id="content">
            
                
            
                
                <div class="pub-div">
                    
                    <div class="pub-img-div">
                        <img class="pub-img" src="/pubE/fig1.jpg" alt="">
                    </div>
                    <div class="pub-descrp">
                        Qiudan Zhang, Xu Wang*, Shiqi Wang, Zhenhao Sun, Sam Kwong, Jianmin Jiang, "<a href="https://Xiaotong713986.github.io/pub/pube/"> Learning to Explore Saliency for Stereoscopic Videos via Component-Based Interaction</a>", 
                        IEEE Transactions on Image Processing, 2020.
                        <p>In this paper, we devise a saliency prediction model for stereoscopic videos that learns to explore saliency inspired by the component-based interactions including spatial, temporal, as well as depth cues. The model first takes advantage of specific structure of 3D residual network (3D-ResNet) to model the saliency driven by spatio-temporal coherence from consecutive frames. Subsequently, the saliency inferred by implicit-depth is automatically derived based on the displacement correlation between left and right views by leveraging a deep convolutional network (ConvNet). Finally, a component-wise refinement network is devised to produce final saliency maps over time by aggregating saliency distributions obtained from multiple components. In order to further facilitate research towards stereoscopic video saliency, we create a new dataset including 175 stereoscopic video sequences with diverse content, as well as their dense eye fixation annotations. Extensive experiments support that our proposed model can achieve superior performance compared to the state-of-the-art methods on all publicly available eye fixation datasets.</p>
                    </div>
                    <div class="clear-float"></div>
                </div>
                
            
                
                <div class="pub-div">
                    
                    <div class="pub-img-div">
                        <img class="pub-img" src="/pubD/fig1.jpg" alt="">
                    </div>
                    <div class="pub-descrp">
                        Xu Wang*, Zhenhao Sun, Qiudan Zhang, Yuming Fang, Lin Ma, Shiqi Wang, Sam Kwong, "<a href="https://Xiaotong713986.github.io/pub/pubd/">Multi-Exposure Decomposition-Fusion Model for High Dynamic Range Image Saliency Detection</a>", 
                        IEEE Transactions on Circuits and Systems for Video Technology, 2020.
                        <p>High dynamic range (HDR) imaging techniques have witnessed a great improvement in the past few decades. However, saliency detection task on HDR content is still far from well explored. In this paper, we introduce a multi-exposure decomposition-fusion model for HDR image saliency detection inspired by the brightness adaption mechanism. The proposed model is composed of three modules. Firstly, a decomposition module converts the input raw HDR image into a stack of LDR images by uniformly sampling the exposure time range. Secondly, a saliency region proposal network is employed to generate the candidate saliency maps for each LDR image in the exposure stack. Finally, an uncertainty weighting based fusion algorithm is applied to generate the overall saliency map for the input HDR image by merging the obtained LDR saliency maps. Extensive experiments show that our proposed model achieves superior performance compared with the state-of-the-art methods on the existing HDR eye fixation databases. The source code of the proposed model are made publicly available at https://github.com/sunnycia/DFHSal.</p>
                    </div>
                    <div class="clear-float"></div>
                </div>
                
            
                
                <div class="pub-div">
                    
                    <div class="pub-img-div">
                        <img class="pub-img" src="/pubC/fig1.jpg" alt="">
                    </div>
                    <div class="pub-descrp">
                        Xuanzheng Wen, Xu Wang*, Junhui Hou, Lin Ma, Yu Zhou, "<a href="https://Xiaotong713986.github.io/pub/pubc/">Lossy Geometry Compression of 3D Point Cloud Data via An Adaptive Octree-Guided Network</a>", 
                        IEEE International Conference on Multimedia and Expo (ICME), 2020.
                        <p>In this paper, we propose a deep learning based framework for point cloud geometry lossy compression via hybrid representation of point cloud. First, the input raw 3D point cloud data is adaptively decomposed into non-overlapping local patches through adaptive Octree decomposition and clustering. Second, a framework of point cloud auto-encoder network with quantization layer is proposed for learning compact latent feature representation from each patch. Specifically, the proposed point cloud auto-encoder networks with different input size are trained for achieving optimal rate-distortion (RD) performance. Final, bitstream specifications of proposed compression systems with additional signaled meta-data and header information are designed to support parallel decoding and successive reconstruction. Experimental results shows that our proposed method can achieve 40.20% bitrate saving in average than the existing standard Geometry based Point Cloud Compression (G-PCC) codec.</p>
                    </div>
                    <div class="clear-float"></div>
                </div>
                
            
                
                <div class="pub-div">
                    
                    <div class="pub-img-div">
                        <img class="pub-img" src="/firstB/overview.jpg" alt="">
                    </div>
                    <div class="pub-descrp">
                        Xu Wang*, Jingming He, Lin Ma, "<a href="https://Xiaotong713986.github.io/pub/firstb/">Exploiting Local and Global Structure for Point Cloud Semantic Segmentation with Contextual Point Representations</a>", 
                        Thirty-third Conference on Neural Information Processing Systems (NeurIPS2019), 2019.
                        <p>In this paper, we propose one novel model for point cloud semantic segmentation, which exploits both the local and global structures within the point cloud based on the contextual point representations. Specifically, we enrich each point representation by performing one novel gated fusion on the point itself and its contextual points. Afterwards, based on the enriched representation, we propose one novel graph pointnet module, relying on the graph attention block to dynamically compose and update each point representation within the local point cloud structure. Finally, we resort to the spatial-wise and channel-wise attention strategies to exploit the point cloud global structure and thereby yield the resulting semantic label for each point. Extensive results on the public point cloud databases, namely the S3DIS and ScanNet datasets, demonstrate the effectiveness of our proposed model, outperforming the state-of-the-art approaches. </p>
                    </div>
                    <div class="clear-float"></div>
                </div>
                
            
                
                <div class="pub-div">
                    
                    <div class="pub-img-div">
                        <img class="pub-img" src="/firstA/Arch_overview.jpg" alt="">
                    </div>
                    <div class="pub-descrp">
                        Qiudan Zhang, Xu Wang*, Shiqi Wang, Shikai Li, Sam Kwong, Jianmin Jiang, "<a href="https://Xiaotong713986.github.io/pub/firsta/">Learning to Explore Intrinsic Saliency for Stereoscopic Video</a>", 
                        IEEE Conference on Computer Vision and Pattern Recognition (CVPR2019), 2019.
                        <p>In this paper, we devise a saliency prediction model for stereoscopic videos that learns to explore saliency inspired by the component-based interactions including spatial, temporal, as well as depth cues. The model first takes advantage of specific structure of 3D residual network (3D-ResNet) to model the saliency driven by spatio-temporal coherence from consecutive frames. Subsequently, the saliency inferred by implicit-depth is automatically derived based on the displacement correlation between left and right views by leveraging a deep convolutional network (ConvNet). Finally, a component-wise refinement network is devised to produce final saliency maps over time by aggregating saliency distributions obtained from multiple components. In order to further facilitate research towards stereoscopic video saliency, we create a new dataset including 175 stereoscopic video sequences with diverse content, as well as their dense eye fixation annotations. Extensive experiments support that our proposed model can achieve superior performance compared to the state-of-the-art methods on all publicly available eye fixation datasets.</p>
                    </div>
                    <div class="clear-float"></div>
                </div>
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
        </div>
<footer class="site-footer">
    <p class="copyright">© 2022 FMC-IML-Lab</p>
    
</footer></body>
</html>